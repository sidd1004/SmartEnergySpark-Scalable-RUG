execution.type = "local"
appliance.testing.data = "/opt/spark/input/testing/"
appliance.testing.data = ${?TESTING_DATA_HOME}
appliance.streamtest.data = "/opt/spark/streaminput/"
appliance.streamtest.data = ${?STREAM_TESTING_DATA_HOME}
spark.master = "cluster"
algorithm.type = "knn"
algorithm.type = ${?ALGORITHM_TYPE}
continuous.training = true
continuous.training = ${?CONTINUOUS_TRAINING}
maximum.itr = 3
maximum.itr = ${?MAXIMUM_ITR}
kvalue = 15
kvalue = ${?KVALUE}
cassandra.datagen {
  hosts = "cassandra"
  hosts = ${?CASSANDRA_DATAGEN_HOSTS}
  port = 9042
  port = ${?CASSANDRA_DATAGEN_PORT}
  keyspace = "applicationlifecycle"
  keyspace = ${?CASSANDRA_DATAGEN_KEYSPACE}
  replication = 1
  replication = ${?CASSANDRA_DATAGEN_KEYSPACE_REPLICATION}
  table_name = "training"
  table_name = ${?CASSANDRA_DATAGEN_TABLE_NAME}
  table {
    schema = "prosumerID text PRIMARY KEY, values text"
    schema = ${?CASSANDRA_DATAGEN_TABLE_SCHEMA}
  }
}

kafka {
  url = "my-cluster-kafka-bootstrap:9092"
  url = ${?KAFKA_URL}
  topic = "testtopic"
  topic = ${?KAFKA_TOPIC}
  group.id = "wacc"
  group.id = ${?KAFKA_GROUP_ID}
  value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
  value.deserializer = ${?KAFKA_VALUE_DESERIALIZER}
  key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
  key.deserializer = ${?KAFKA_KEY_DESERIALIZER}
  application.id = "wacc_kafka_streaming"
  application.id = ${?APPLICATION_ID}
  consumer.timeout = 3000
  consumer.timeout = ${?KAFKA_CONSUMER_TIMEOUT}
  streamquery {
    topic = "StreamPipeline"
    startingOffset = "latest"
    checkpointLocation = "/opt/spark/checkpoints/"
  }
  batchquery {
    topic = "BatchPipeline"
    startingOffset = "latest"
    checkpointLocation = "/opt/spark/checkpoints/"
  }
}
